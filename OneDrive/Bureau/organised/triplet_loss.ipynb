{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_df= pd.read_csv('triplet_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_df=triplet_loss_df[['image','collection_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>collection_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image0.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image1.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image2.jpg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image3.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image4.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image  collection_id\n",
       "0  image0.jpg              2\n",
       "1  image1.jpg              6\n",
       "2  image2.jpg             11\n",
       "3  image3.jpg             16\n",
       "4  image4.jpg             16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been processed and saved to processed_images\n"
     ]
    }
   ],
   "source": [
    "def resize_and_pad_image(img, desired_height, desired_width, pad_color=(255, 255, 255)):\n",
    "    # Convert grayscale images to RGB\n",
    "    if img.mode == 'L':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    original_width, original_height = img.size\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    # Determine new dimensions based on aspect ratio\n",
    "    if original_width / original_height > desired_width / desired_height:\n",
    "        new_width = desired_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = desired_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image to new dimensions\n",
    "    image_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Calculate padding to achieve desired dimensions\n",
    "    padding_left = (desired_width - new_width) // 2\n",
    "    padding_right = desired_width - new_width - padding_left\n",
    "    padding_top = (desired_height - new_height) // 2\n",
    "    padding_bottom = desired_height - new_height - padding_top\n",
    "\n",
    "    # Pad the resized image to achieve desired dimensions\n",
    "    image_padded = ImageOps.expand(image_resized, border=(padding_left, padding_top, padding_right, padding_bottom), fill=pad_color)\n",
    "\n",
    "    return image_padded\n",
    "\n",
    "\n",
    "\n",
    "# Path to the directory containing images\n",
    "source_dir = 'images_full'\n",
    "# Path to the directory where the processed images will be saved\n",
    "target_dir = 'processed_images'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Desired dimensions after padding\n",
    "desired_height = 512\n",
    "desired_width = 512\n",
    "\n",
    "# Process each image in the directory\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp')):  # Check for image files\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        img = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB\n",
    "\n",
    "        # Resize and pad the image\n",
    "        padded_image = resize_and_pad_image(img, desired_height, desired_width)\n",
    "\n",
    "        # Save the processed image to the new directory\n",
    "        save_path = os.path.join(target_dir, filename)\n",
    "        padded_image.save(save_path)\n",
    "\n",
    "print(\"All images have been processed and saved to\", target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where the images are stored\n",
    "base_directory = 'processed_images'\n",
    "\n",
    "# Prepend the base directory to each image filename in the dataframe\n",
    "triplet_loss_df['image'] = triplet_loss_df['image'].apply(lambda x: os.path.join(base_directory, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>collection_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_images\\image0.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_images\\image1.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_images\\image2.jpg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_images\\image3.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processed_images\\image4.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  collection_id\n",
       "0  processed_images\\image0.jpg              2\n",
       "1  processed_images\\image1.jpg              6\n",
       "2  processed_images\\image2.jpg             11\n",
       "3  processed_images\\image3.jpg             16\n",
       "4  processed_images\\image4.jpg             16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_uniform_image_sizes(base_directory):\n",
    "    # Initialize a variable to store the size of the first image\n",
    "    first_image_size = None\n",
    "    uniform_size = True\n",
    "\n",
    "    # Loop through all the images in the directory\n",
    "    for filename in os.listdir(base_directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(base_directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # Set the size of the first image\n",
    "            if first_image_size is None:\n",
    "                first_image_size = img.size\n",
    "                print(f\"Expected size based on the first image: {first_image_size}\")\n",
    "\n",
    "            # Check the size of each image against the first image's size\n",
    "            if img.size != first_image_size:\n",
    "                print(f\"{filename}: Size {img.size} does NOT match {first_image_size}\")\n",
    "                uniform_size = False\n",
    "\n",
    "    if uniform_size:\n",
    "        print(\"All images have the same size.\")\n",
    "    else:\n",
    "        print(\"Not all images have the same size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size based on the first image: (512, 512)\n",
      "All images have the same size.\n"
     ]
    }
   ],
   "source": [
    "check_uniform_image_sizes(base_directory='processed_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def augmentation(image):\n",
    "    original_size = (512, 512)  #  all images are 512x512\n",
    "\n",
    "    # Define transformations with white fill where applicable\n",
    "    crop = transforms.Compose([\n",
    "        transforms.CenterCrop((int(original_size[0] * 0.8), int(original_size[1] * 0.8))),\n",
    "        transforms.Pad((int(original_size[1] * 0.1), int(original_size[0] * 0.1)), fill=(255, 255, 255)),\n",
    "        transforms.Resize(original_size)\n",
    "    ])\n",
    "\n",
    "    random_horizontal_flip = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0)\n",
    "    ])\n",
    "\n",
    "    random_rotation = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(50, fill=(255, 255, 255)),\n",
    "        transforms.Resize(original_size)\n",
    "    ])\n",
    "\n",
    "    random_vertical_flip = transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(p=1.0)\n",
    "    ])\n",
    "\n",
    "    random_affine = transforms.Compose([\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10, fill=(255, 255, 255)),\n",
    "        transforms.Resize(original_size)\n",
    "    ])\n",
    "\n",
    "    random_perspective = transforms.Compose([\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0, fill=(255, 255, 255)),\n",
    "        transforms.Resize(original_size)\n",
    "    ])\n",
    "\n",
    "    gaussian_blur = transforms.Compose([\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))\n",
    "    ])\n",
    "\n",
    "    random_grayscale = transforms.Compose([\n",
    "        transforms.RandomGrayscale(p=1.0)\n",
    "    ])\n",
    "\n",
    "    random_invert = transforms.Compose([\n",
    "        transforms.RandomInvert(p=1.0)\n",
    "    ])\n",
    "\n",
    "    # Apply transformations to the image\n",
    "    image_crop = crop(image)\n",
    "    image_random_horizontal_flip = random_horizontal_flip(image)\n",
    "    image_random_rotation = random_rotation(image)\n",
    "    image_random_vertical_flip = random_vertical_flip(image)\n",
    "    image_random_affine = random_affine(image)\n",
    "    image_random_perspective = random_perspective(image)\n",
    "    image_gaussian_blur = gaussian_blur(image)\n",
    "    image_random_grayscale = random_grayscale(image)\n",
    "    image_random_invert = random_invert(image)\n",
    "\n",
    "    # Return the augmented images\n",
    "    return (image_crop, image_random_horizontal_flip, image_random_rotation,\n",
    "            image_random_vertical_flip, image_random_affine,\n",
    "            image_random_perspective, image_gaussian_blur, image_random_grayscale,\n",
    "            image_random_invert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting images:   2%|▏         | 2/100 [00:00<00:10,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot augment image processed_images\\image0.jpg: [Errno 2] No such file or directory: 'processed_images\\\\image0.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting images: 100%|██████████| 100/100 [00:12<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directory to save augmented images\n",
    "augmented_images_dir = 'augmented_images'\n",
    "os.makedirs(augmented_images_dir, exist_ok=True)\n",
    "\n",
    "# List to store the new rows for the DataFrame\n",
    "new_rows_r = []\n",
    "\n",
    "# Loop through each image in the DataFrame and augment the images\n",
    "for index, row in tqdm(triplet_loss_df.iterrows(), desc=\"Augmenting images\", total=triplet_loss_df.shape[0]):\n",
    "    image_path = row['image']\n",
    "    class_id = row['collection_id']\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        augmented_images = augmentation(image)\n",
    "\n",
    "        for i, augmented_image in enumerate(augmented_images):\n",
    "            # Save the augmented image\n",
    "            augmented_image_path = os.path.join(augmented_images_dir, f'augmented_{class_id}_{index}_{i}.jpg')\n",
    "            augmented_image.save(augmented_image_path)\n",
    "\n",
    "            # Add new row for augmented data\n",
    "            new_row = {'collection_id': class_id, 'image': augmented_image_path}\n",
    "            new_rows_r.append(new_row)\n",
    "\n",
    "    except (IOError, UnidentifiedImageError) as e:\n",
    "        print(f\"Cannot augment image {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected size based on the first image: (512, 512)\n",
      "All images have the same size.\n"
     ]
    }
   ],
   "source": [
    "check_uniform_image_sizes(base_directory='augmented_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.DataFrame(new_rows_r)\n",
    "triplet_loss_df_aug = pd.concat([triplet_loss_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe\n",
    "triplet_loss_df_aug.to_csv('augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image</th>\n",
       "      <th>collection_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_images\\image0.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>processed_images\\image1.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>processed_images\\image2.jpg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>processed_images\\image3.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>processed_images\\image4.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        image  collection_id\n",
       "0           0  processed_images\\image0.jpg              2\n",
       "1           1  processed_images\\image1.jpg              6\n",
       "2           2  processed_images\\image2.jpg             11\n",
       "3           3  processed_images\\image3.jpg             16\n",
       "4           4  processed_images\\image4.jpg             16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('augmented_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_df_aug=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image</th>\n",
       "      <th>collection_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_images\\image0.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>processed_images\\image1.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>processed_images\\image2.jpg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>processed_images\\image3.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>processed_images\\image4.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        image  collection_id\n",
       "0           0  processed_images\\image0.jpg              2\n",
       "1           1  processed_images\\image1.jpg              6\n",
       "2           2  processed_images\\image2.jpg             11\n",
       "3           3  processed_images\\image3.jpg             16\n",
       "4           4  processed_images\\image4.jpg             16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss_df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_triplet(index, df, unique_collections):\n",
    "\n",
    "\n",
    "  # Get anchor id based on index\n",
    "  anchor_id = df.iloc[index]['collection_id']\n",
    "  same_collection_df = df[df['collection_id'] == anchor_id]\n",
    "\n",
    "  # Get anchor image\n",
    "  anchor_idx = index\n",
    "  anchor_image_path = same_collection_df.loc[anchor_idx, 'image']\n",
    "  anchor_image = Image.open(anchor_image_path).convert('RGB')\n",
    "\n",
    "\n",
    "  positive_indices = same_collection_df.index.difference([anchor_idx])\n",
    "  positive_image_path = same_collection_df.loc[random.choice(positive_indices), 'image']\n",
    "  positive_image = Image.open(positive_image_path).convert('RGB')\n",
    "\n",
    "  # Select a negative image from a different collection\n",
    "  negative_id = np.random.choice(unique_collections[unique_collections != anchor_id])\n",
    "  negative_df = df[df['collection_id'] == negative_id]\n",
    "  i = np.random.choice(negative_df.shape[0])\n",
    "  negative_image_path = negative_df.iloc[i]['image']\n",
    "  negative_image = Image.open(negative_image_path).convert('RGB')\n",
    "\n",
    "\n",
    "  return anchor_image, positive_image, negative_image, anchor_id, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hard_triplet(index, df, cache_emb, margin=0.2):\n",
    "    anchor_embedding = cache_emb.get_index_emb(index).view(1, -1)\n",
    "    all_embeddings = cache_emb.all_embeddings\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    dists = torch.cdist(anchor_embedding, all_embeddings, p=2).squeeze()\n",
    "\n",
    "    # Get the sorted distances and sorted indices (excluding the anchor itself)\n",
    "    sorted_dists, sorted_indices = torch.sort(dists)\n",
    "    anchor_idx = index\n",
    "    anchor_id = df.iloc[index]['collection_id']\n",
    "    same_collection_df = df[df['collection_id'] == anchor_id]\n",
    "    positive_indices = same_collection_df.index.difference([anchor_idx])\n",
    "\n",
    "    # Find the hard positive example (same collection, closest Euclidean distance)\n",
    "    positive_dists = dists[positive_indices]\n",
    "    hard_positive = torch.argmin(positive_dists)\n",
    "    hard_positive_id = positive_indices[hard_positive]\n",
    "    positive_image_path = df.loc[hard_positive_id, 'image']\n",
    "    hard_positive_image = Image.open(positive_image_path).convert('RGB')\n",
    "\n",
    "    # Find the semi-hard negative example (different collection)\n",
    "    different_collection_df = df[df['collection_id'] != anchor_id]\n",
    "    negative_indices = different_collection_df.index\n",
    "\n",
    "    positive_distance = positive_dists[hard_positive].item()\n",
    "\n",
    "    # Select semi-hard negative (distance greater than positive but within the margin)\n",
    "    semi_hard_negative_indices = [idx for idx in negative_indices if positive_distance < dists[idx].item() < positive_distance + margin]\n",
    "    if semi_hard_negative_indices:\n",
    "        semi_hard_negative_id = semi_hard_negative_indices[0]\n",
    "    else:\n",
    "        # If no semi-hard negative found within the margin, fall back to the closest negative outside the margin\n",
    "        negative_dists = dists[negative_indices]\n",
    "        semi_hard_negative_id = negative_indices[torch.argmin(negative_dists)]\n",
    "\n",
    "    negative_image_path = df.loc[semi_hard_negative_id, 'image']\n",
    "    semi_hard_negative_image = Image.open(negative_image_path).convert('RGB')\n",
    "\n",
    "    return hard_positive_image, semi_hard_negative_image, anchor_id, index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding_cache():\n",
    "  def __init__(self):\n",
    "    self.emb={}\n",
    "    self.all_embeddings={}\n",
    "\n",
    "  def get_index_emb(self, index):\n",
    "    return self.all_embeddings[index]\n",
    "\n",
    "  def store_embedding(self, model, train_loader ):\n",
    "\n",
    "    for batch in train_loader:\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "\n",
    "        emb_anchor=model(batch[0])\n",
    "\n",
    "      embedding_batch={index.item():embedding for index, embedding in zip(batch[-1],emb_anchor)}\n",
    "      self.all_embeddings={**embedding_batch, **self.all_embeddings}\n",
    "    sorted_embeddings = dict(sorted(self.all_embeddings.items()))\n",
    "    sorted_embeddings_values=sorted_embeddings.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_emb=embedding_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetworkDataset(Dataset):\n",
    "    def __init__(self, df, cache_emb, transform=None, use_hard_triplets=False):\n",
    "        self.df = df\n",
    "        self.cache_emb = cache_emb\n",
    "        self.transform = transform\n",
    "        self.use_hard_triplets = use_hard_triplets\n",
    "        self.unique_collections = df['collection_id'].unique()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.use_hard_triplets:\n",
    "            anchor_image, positive_image, negative_image, anchor_id, index = generate_hard_triplet(index, self.df, self.cache_emb)\n",
    "\n",
    "        else:\n",
    "            anchor_image, positive_image, negative_image, anchor_id, index = generate_random_triplet(index, self.df, self.unique_collections)\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "            positive_image = self.transform(positive_image)\n",
    "            negative_image = self.transform(negative_image)\n",
    "\n",
    "        return anchor_image, positive_image, negative_image, anchor_id, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((512, 512)),  # Resize all images to 928 × 384\n",
    "    transforms.ToTensor(),          # Convert image data to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePreprocessLayer(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(LearnablePreprocessLayer, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=1, padding=0, bias=True),  # input_channels 3, 16 output channels, 1x1 kernel\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.BatchNorm2d(16),  # BatchNorm on the output channels\n",
    "        )\n",
    "        self.layers2 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "        )\n",
    "        self.layers3 = nn.Sequential(\n",
    "            nn.Conv2d(16 + input_channels, input_channels, kernel_size=1, padding=0, bias=True),  # Adjusted input channels\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.layers1(x)\n",
    "        x2 = self.layers2(x)\n",
    "        x_concat = torch.cat((x1, x2), 1)\n",
    "        x3 = self.layers3(x_concat)\n",
    "        return x3\n",
    "\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.ap = nn.AdaptiveAvgPool2d(size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ap = self.ap(x)\n",
    "        x_mp = self.mp(x)\n",
    "        return torch.cat([x_mp, x_ap], 1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(model_name: str, freeze_backbone: bool = True, input_channels=3):\n",
    "    model = getattr(torchvision.models, model_name)(pretrained=True)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Add the LearnablePreprocessLayer\n",
    "    preprocessing = LearnablePreprocessLayer(input_channels=input_channels)\n",
    "\n",
    "    # Replace the first layer of the model with the LearnablePreprocessLayer\n",
    "    if hasattr(model, 'conv1'):\n",
    "        original_first_conv = model.conv1\n",
    "        model.conv1 = nn.Sequential(\n",
    "            preprocessing,\n",
    "            original_first_conv\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{model_name} architecture does not have a 'conv1' layer\")\n",
    "\n",
    "    # Modify the fully connected layer (fc) assuming the model uses a classifier named 'fc'\n",
    "    if hasattr(model, 'fc'):\n",
    "        num_features = model.fc.in_features\n",
    "        model.avgpool = AdaptiveConcatPool2d()\n",
    "        model.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.BatchNorm1d(num_features * 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(num_features * 2, 512, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            L2Norm(dim=1)\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"This model architecture is not supported yet.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khadi\\.conda\\envs\\openmmlab\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Khadi\\.conda\\envs\\openmmlab\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): LearnablePreprocessLayer(\n",
      "      (layers1): Sequential(\n",
      "        (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layers2): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layers3): Sequential(\n",
      "        (0): Conv2d(19, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  )\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveConcatPool2d(\n",
      "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
      "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten()\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (8): L2Norm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_model('resnet50', freeze_backbone=True, input_channels=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique collection ids\n",
    "unique_collection_ids = data['collection_id'].unique()\n",
    "\n",
    "train_df_list = []\n",
    "val_df_list = []\n",
    "\n",
    "\n",
    "\n",
    "for unique_collection_id in unique_collection_ids:\n",
    "    collection_images = data[data['collection_id'] == unique_collection_id]\n",
    "    train_images, val_images = train_test_split(\n",
    "        collection_images, test_size=2/len(collection_images), random_state=42\n",
    "    )\n",
    "    train_df_list.append(train_images)\n",
    "    val_df_list.append(val_images)\n",
    "\n",
    "train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
    "val_df = pd.concat(val_df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripletNetworkDataset(train_df, transform=transform, cache_emb=cache_emb)\n",
    "val_dataset = TripletNetworkDataset(val_df, transform=transform,cache_emb=cache_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the triplet loss\n",
    "triplet_loss = nn.TripletMarginLoss(margin=0.2, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "num_epochs = 1  # Reduce the number of epochs for quick testing\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float('inf')  # To keep track of the best loss\n",
    "loss_history = []  # Initialize loss history list\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    num_batches = 0  # Initialize the number of batches processed\n",
    "    train_loader_progress = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch in train_loader_progress:\n",
    "        # Move data to the same device as the model\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        anchor_imgs, positive_imgs, negative_imgs, labels, index = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embeddings = model(anchor_imgs)\n",
    "        positive_embeddings = model(positive_imgs)\n",
    "        negative_embeddings = model(negative_imgs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar with the current loss\n",
    "        train_loader_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Save embeddings\n",
    "    cache_emb.store_embedding(model, train_loader, device)\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    if num_batches > 0:  # Ensure division by zero doesn't occur\n",
    "        average_loss = running_loss / num_batches\n",
    "    else:\n",
    "        average_loss = 0\n",
    "    loss_history.append(average_loss)\n",
    "    print(f\"Average Loss for Epoch {epoch + 1}: {average_loss:.4f}\")\n",
    "\n",
    "    # Save the model if the average loss is the best\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch + 1}_loss_{average_loss:.4f}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Check embeddings\n",
    "print(\"Embeddings stored: \", len(cache_emb.embeddings))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
